{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Book\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Book(bookID: Int,\n",
    "                title: String,\n",
    "                authors: String,\n",
    "                averageRating: Float,\n",
    "                isbn: String,\n",
    "                isbn13: String,\n",
    "                languageCode:String,\n",
    "                numPages: Int,\n",
    "                ratingsCount: Int,\n",
    "                textReviewsCount: Int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not everything can run in the Jupyter Notebook\n",
    "\n",
    "https://github.com/Valassis-Digital-Media/spylon-kernel/issues/40\n",
    "\n",
    "Note that we cannot run all the example in the jupyter notebook due to ongoing ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasons for a `Dataset`\n",
    "* Operations require functional programming solutions\n",
    "* Rigorous Type Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Dataset`\n",
    "\n",
    "* `DataFrame` is a collection of `DataSet[Row]`\n",
    "* Datasets are a strictly Java Virtual Machine (JVM) language feature\n",
    "    * Work only with Scala and Java\n",
    "* Can use an internal model representation structure \n",
    "  * For Scala that would be a `case class`\n",
    "  * For Java that would be a Java Bean\n",
    "  * There is slower performance with `DataSet` than a `DataFrame` due to conversion to custom Java objects\n",
    "  * When using `case class`es it trivial to reuse them for both distributed and local workloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets are DataFrames\n",
    "\n",
    "* When reading in the data, `DataFrames` are `Dataset[Row]`\n",
    "* This is done as a type alias `type DataFrame = Dataset[Row]`\n",
    "* Therefore we can perform some functional programming like:\n",
    "  * `map`\n",
    "  * `flatMap`\n",
    "  * `filter`\n",
    "  * `foreach`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the `case class`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "bookSchema: org.apache.spark.sql.types.StructType = StructType(StructField(bookID,IntegerType,false), StructField(title,StringType,false), StructField(authors,StringType,false), StructField(average_rating,FloatType,false), StructField(isbn,StringType,false), StructField(isbn13,StringType,false), StructField(language_code,StringType,false), StructField(# num_pages,IntegerType,false), StructField(ratings_count,IntegerType,false), StructField(text_reviews_count,IntegerType,false))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val bookSchema = new StructType(Array(\n",
    "   new StructField(\"bookID\", IntegerType, false),\n",
    "   new StructField(\"title\", StringType, false),\n",
    "   new StructField(\"authors\", StringType, false),\n",
    "   new StructField(\"average_rating\", FloatType, false),\n",
    "   new StructField(\"isbn\", StringType, false),\n",
    "   new StructField(\"isbn13\", StringType, false),\n",
    "   new StructField(\"language_code\", StringType, false),\n",
    "   new StructField(\"# num_pages\", IntegerType, false),\n",
    "   new StructField(\"ratings_count\", IntegerType, false),\n",
    "   new StructField(\"text_reviews_count\", IntegerType, false)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating column names to match the `case class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columnNames: Seq[String] = List(bookID, title, authors, averageRating, isbn, isbn13, languageCode, numPages, ratingsCount, textReviewsCount)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columnNames = Seq(\"bookID\", \"title\", \"authors\",\n",
    "      \"averageRating\", \"isbn\",\n",
    "      \"isbn13\", \"languageCode\", \"numPages\", \"ratingsCount\",\n",
    "      \"textReviewsCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "|bookID|               title|             authors|averageRating|      isbn|       isbn13|languageCode|numPages|ratingsCount|textReviewsCount|\n",
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "|     1|Harry Potter and ...|J.K. Rowling-Mary...|         4.56|0439785960|9780439785969|         eng|     652|     1944099|           26249|\n",
      "|     2|Harry Potter and ...|J.K. Rowling-Mary...|         4.49|0439358078|9780439358071|         eng|     870|     1996446|           27613|\n",
      "|     3|Harry Potter and ...|J.K. Rowling-Mary...|         4.47|0439554934|9780439554930|         eng|     320|     5629932|           70390|\n",
      "|     4|Harry Potter and ...|        J.K. Rowling|         4.41|0439554896|9780439554893|         eng|     352|        6267|             272|\n",
      "|     5|Harry Potter and ...|J.K. Rowling-Mary...|         4.55|043965548X|9780439655484|         eng|     435|     2149872|           33964|\n",
      "|     8|Harry Potter Boxe...|J.K. Rowling-Mary...|         4.78|0439682584|9780439682589|         eng|    2690|       38872|             154|\n",
      "|     9|Unauthorized Harr...|W. Frederick Zimm...|         3.69|0976540606|9780976540601|       en-US|     152|          18|               1|\n",
      "|    10|Harry Potter Coll...|        J.K. Rowling|         4.73|0439827604|9780439827607|         eng|    3342|       27410|             820|\n",
      "|    12|The Ultimate Hitc...|       Douglas Adams|         4.38|0517226952|9780517226957|         eng|     815|        3602|             258|\n",
      "|    13|The Ultimate Hitc...|       Douglas Adams|         4.38|0345453743|9780345453747|         eng|     815|      240189|            3954|\n",
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.Dataset\n",
       "dataset: org.apache.spark.sql.DataFrame = [bookID: int, title: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "val dataset = spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"encoding\", \"UTF-8\")\n",
    "      .schema(bookSchema)\n",
    "      .csv(\"../data/books.csv\")\n",
    "      .toDF(columnNames:_*) //Rename Columns\n",
    "      .na.drop()            //Drop NA Values\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "|bookID|               title|             authors|averageRating|      isbn|       isbn13|languageCode|numPages|ratingsCount|textReviewsCount|\n",
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "|  4381|      Fahrenheit 451|Ray Bradbury-Alfr...|         3.98|0307347974|9780307347978|         spa|     175|      690801|           14489|\n",
      "|  4382|      Fahrenheit 451|Ray Bradbury-Chri...|         3.98|078617627X|9780786176274|         eng|       5|         471|             142|\n",
      "|  7656|      Fahrenheit 451|        Ray Bradbury|         3.98|8445074873|9788445074879|         eng|     186|        5733|             613|\n",
      "| 32971|      Fahrenheit 451|        Ray Bradbury|         3.98|0965020592|9780965020596|         eng|     190|         185|              26|\n",
      "| 32972|      Fahrenheit 451|        Ray Bradbury|         3.98|0345023021|9780345023025|         eng|     147|         208|              30|\n",
      "| 32973|      Fahrenheit 451|Ray Bradbury-Fran...|         3.98|9505470010|9789505470013|         spa|     263|         173|              23|\n",
      "| 37683|      Fahrenheit 451|Ray Bradbury-Alfr...|         3.98|8497930053|9788497930055|         spa|     176|         574|              64|\n",
      "| 40582|Michael Moore's F...| Robert Brent Toplin|         3.38|0700614524|9780700614523|         eng|     161|           8|               1|\n",
      "| 40694|Ray Bradbury's Fa...|        Harold Bloom|         4.18|0791059294|9780791059296|         eng|     147|         938|              47|\n",
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.filter(_.getAs[String](\"title\").contains(\"Fahrenheit\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "|bookID|               title|             authors|averageRating|      isbn|       isbn13|languageCode|numPages|ratingsCount|textReviewsCount|\n",
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "|     1|Harry Potter and ...|J.K. Rowling-Mary...|         4.56|0439785960|9780439785969|         eng|     652|     1944099|           26249|\n",
      "|     2|Harry Potter and ...|J.K. Rowling-Mary...|         4.49|0439358078|9780439358071|         eng|     870|     1996446|           27613|\n",
      "|     3|Harry Potter and ...|J.K. Rowling-Mary...|         4.47|0439554934|9780439554930|         eng|     320|     5629932|           70390|\n",
      "|     4|Harry Potter and ...|        J.K. Rowling|         4.41|0439554896|9780439554893|         eng|     352|        6267|             272|\n",
      "|     5|Harry Potter and ...|J.K. Rowling-Mary...|         4.55|043965548X|9780439655484|         eng|     435|     2149872|           33964|\n",
      "|     8|Harry Potter Boxe...|J.K. Rowling-Mary...|         4.78|0439682584|9780439682589|         eng|    2690|       38872|             154|\n",
      "|     9|Unauthorized Harr...|W. Frederick Zimm...|         3.69|0976540606|9780976540601|       en-US|     152|          18|               1|\n",
      "|    10|Harry Potter Coll...|        J.K. Rowling|         4.73|0439827604|9780439827607|         eng|    3342|       27410|             820|\n",
      "|    12|The Ultimate Hitc...|       Douglas Adams|         4.38|0517226952|9780517226957|         eng|     815|        3602|             258|\n",
      "|    13|The Ultimate Hitc...|       Douglas Adams|         4.38|0345453743|9780345453747|         eng|     815|      240189|            3954|\n",
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.Dataset\n",
       "dataset: org.apache.spark.sql.Dataset[Book] = [bookID: int, title: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* The imports are required for use */\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "val dataset: Dataset[Book] = spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"encoding\", \"UTF-8\")\n",
    "      .schema(bookSchema)\n",
    "      .csv(\"../data/books.csv\")\n",
    "      .toDF(columnNames:_*) //Rename Columns\n",
    "      .na.drop()            //Drop NA Values\n",
    "      .as[Book]             //Conversion to Case Class\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some rudimentary functional programming \n",
    "\n",
    "* While it is better to run with our own custom types, we can also perform functional programming using `Row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost, executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost, executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)",
      "  at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:745)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:704)",
      "  ... 41 elided",
      "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:121)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "dataset.map(_.authors).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting to a `case class`\n",
    "\n",
    "* We can also create a `case class` of the representation of the `Row`\n",
    "* We can use the schema we created as a reference for our new `case class`\n",
    "\n",
    "```\n",
    "new StructField(\"bookID\", IntegerType, false),\n",
    "new StructField(\"title\", StringType, false),\n",
    "new StructField(\"authors\", StringType, false),\n",
    "new StructField(\"average_rating\", FloatType, false),\n",
    "new StructField(\"isbn\", StringType, false),\n",
    "new StructField(\"isbn13\", StringType, false),\n",
    "new StructField(\"language_code\", StringType, false),\n",
    "new StructField(\"num_pages\", IntegerType, false),\n",
    "new StructField(\"ratings_count\", IntegerType, false),\n",
    "new StructField(\"text_reviews_count\", IntegerType, false)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Book\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Book (bookID:Long, \n",
    "                      title:String,\n",
    "                      authors:String,\n",
    "                      averageRating:Float,\n",
    "                      isbn:String, \n",
    "                      isbn13:String,\n",
    "                      languageCode:String,\n",
    "                      numPages:Int, \n",
    "                      ratingsCount:Int,\n",
    "                      textReviewsCount:Int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the columns so we can fit it to Scala convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columnNames: Seq[String] = List(bookID, title, authors, averageRating, isbn, isbn13, languageCode, numPages, ratingsCount, textReviewsCount)\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columnNames = Seq(\"bookID\", \"title\", \"authors\", \"averageRating\", \"isbn\",\n",
    "                                  \"isbn13\", \"languageCode\", \"numPages\", \"ratingsCount\",\n",
    "                                  \"textReviewsCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting it our type using `as[T]` \n",
    "\n",
    "* Note that the explicit declaration type `Dataset[BookEntry]` is not required\n",
    "* We are showing it for understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "|bookID|               title|             authors|averageRating|      isbn|       isbn13|languageCode|numPages|ratingsCount|textReviewsCount|\n",
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "|     1|Harry Potter and ...|J.K. Rowling-Mary...|         4.56|0439785960|9780439785969|         eng|     652|     1944099|           26249|\n",
      "|     2|Harry Potter and ...|J.K. Rowling-Mary...|         4.49|0439358078|9780439358071|         eng|     870|     1996446|           27613|\n",
      "|     3|Harry Potter and ...|J.K. Rowling-Mary...|         4.47|0439554934|9780439554930|         eng|     320|     5629932|           70390|\n",
      "|     4|Harry Potter and ...|        J.K. Rowling|         4.41|0439554896|9780439554893|         eng|     352|        6267|             272|\n",
      "|     5|Harry Potter and ...|J.K. Rowling-Mary...|         4.55|043965548X|9780439655484|         eng|     435|     2149872|           33964|\n",
      "|     8|Harry Potter Boxe...|J.K. Rowling-Mary...|         4.78|0439682584|9780439682589|         eng|    2690|       38872|             154|\n",
      "|     9|Unauthorized Harr...|W. Frederick Zimm...|         3.69|0976540606|9780976540601|       en-US|     152|          18|               1|\n",
      "|    10|Harry Potter Coll...|        J.K. Rowling|         4.73|0439827604|9780439827607|         eng|    3342|       27410|             820|\n",
      "|    12|The Ultimate Hitc...|       Douglas Adams|         4.38|0517226952|9780517226957|         eng|     815|        3602|             258|\n",
      "|    13|The Ultimate Hitc...|       Douglas Adams|         4.38|0345453743|9780345453747|         eng|     815|      240189|            3954|\n",
      "+------+--------------------+--------------------+-------------+----------+-------------+------------+--------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "booksDS: org.apache.spark.sql.Dataset[Book] = [bookID: int, title: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val booksDS:Dataset[Book] = spark.read.format(\"csv\")\n",
    "                                      .schema(bookSchema)\n",
    "                                      .option(\"header\", \"true\")\n",
    "                                      .option(\"encoding\", \"UTF-8\")\n",
    "                                      .csv(\"../data/books.csv\")\n",
    "                                      .toDF(columnNames:_*) //Rename Columns\n",
    "                                      .na.drop()            //Drop NA Values\n",
    "                                      .as[Book]             //Conversion to Case Class\n",
    "booksDS.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 33, localhost, executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 33, localhost, executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)",
      "  at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:745)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:704)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:713)",
      "  ... 44 elided",
      "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:121)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "val filtered = booksDS.filter{x => x.title.contains(\"Fahrenheit\")}\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating from a Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Some of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating DataSets with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Functional Programming with DataSets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
