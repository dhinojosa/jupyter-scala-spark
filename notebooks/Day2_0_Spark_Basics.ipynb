{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Basics\n",
    "Authors: Daniel Hinojosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture\n",
    "\n",
    "![Spark Architecture](../images/spark_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason for Spark’s Existence\n",
    "* CPU went from single to multi-core\n",
    "* Hard Drive storage became cheap over time\n",
    "* This allows for processing of data without expense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture\n",
    "* The reason for existence is that one computer is too slow for processing data\n",
    "* A cluster can provide faster processing in parallel.\n",
    "* Spark is separated by:\n",
    "  * A _driver_ process\n",
    "  * An _executor_ process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Driver\n",
    "* The driver system for your application\n",
    "* Maintains information about the application\n",
    "* Responds to external programs\n",
    "* Analyzes work across executors\n",
    "* Distributes work across executors\n",
    "* Schedules work across executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Executor\n",
    "\n",
    "* Executes code assigned to it by the driver\n",
    "* Reports the state of the computation back to the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Extras\n",
    "\n",
    "* **_MLlib_** - Machine Learning with Spark\n",
    "* **_GraphX_** - for Graph Processing\n",
    "* **_SparkR_** - for working with Clusters using R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Manager\n",
    "\n",
    "* Since Spark is a system where it separates work and distributes them it requires multiple machines\n",
    "* Thus, it requires a cluster manager to manage the remote nodes, known as cluster mode\n",
    "* Controls the Physical Machines\n",
    "* Allocates resources to Spark applications\n",
    "* Cluster Managers can either be:\n",
    "  * Sparks in-house cluster manager\n",
    "  * YARN\n",
    "  * Mesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Mode\n",
    "\n",
    "* Instead of remote machines this will run on your internal box\n",
    "* Easy for testing, in house demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Languages Supported\n",
    "\n",
    "* Scala (Spark’s default language)\n",
    "* Python (Does nearly everything that Scala does)\n",
    "* Java (Louder than Scala)\n",
    "* SQL (Spark SQL is compliant SQL to interact with querying data)\n",
    "* R/Spark (The classic Big Data language)\n",
    "\n",
    "For this class/workshop we will be using Scala since it is \n",
    "  less verbose and has other features that Java does not have. It is also\n",
    "  Spark's default language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Intro\n",
    "* Big data processing framework\n",
    "* Variety of packages built upon Spark engine\n",
    "* Contains two APIs\n",
    "  * _Unstructured API_\n",
    "    * Lower Level\n",
    "    * `RDD`\n",
    "    * Accumulators\n",
    "    * Broadcast Variables\n",
    "  * _Structured API_\n",
    "    * Higher Level\n",
    "    * Optimized\n",
    "    * `DataFrame`\n",
    "    * `Dataset`\n",
    "    * `Spark SQL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RDD`\n",
    "* `RDD` stands for _Resilient Distributed Dataset_ it is the underlying data structure in which Spark operates\n",
    "* `RDD` can still be used but they are unoptimized unlike `DataFrame`, `DataSet`, and `SparkSQL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You want to use `DataFrame`, `DataSet`, `SparkSQL` over `RDD`\n",
    "\n",
    "Source: High Performance Spark\n",
    "by Holden Karau; Rachel Warren\n",
    "Published by O'Reilly Media, Inc., 2017\n",
    "\n",
    "Here is a chart which shows the difference in performance with the various data structures in Spark, especially when it comes to aggregations\n",
    "\n",
    "![execution_time.png](../images/execution_time.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
