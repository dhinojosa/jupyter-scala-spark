{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning DataFrames\n",
    "\n",
    "* Much of the data that we manage needs to be continually cleaned\n",
    "* This chapter is dedicated to techniques for cleaning our data for tasks like:\n",
    "  * Reporting\n",
    "  * Machine Learning\n",
    "* Please open a tab that shows [the API for dealing with `null` values](https://spark.apache.org/docs/2.3.0/api/scala/index.html?org/apache/spark/sql/Dataset.html#org.apache.spark.sql.DataFrameNaFunctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing in our  `DataFrame`\n",
    "\n",
    "Here we will bring in our `DataFame` for books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bookID: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- average_rating: float (nullable = true)\n",
      " |-- isbn: string (nullable = true)\n",
      " |-- isbn13: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- num_pages: integer (nullable = true)\n",
      " |-- ratings_count: integer (nullable = true)\n",
      " |-- text_reviews_count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "bookSchema: org.apache.spark.sql.types.StructType = StructType(StructField(bookID,IntegerType,false), StructField(title,StringType,false), StructField(authors,StringType,false), StructField(average_rating,FloatType,false), StructField(isbn,StringType,false), StructField(isbn13,StringType,false), StructField(language_code,StringType,false), StructField(num_pages,IntegerType,false), StructField(ratings_count,IntegerType,false), StructField(text_reviews_count,IntegerType,false))\n",
       "booksDF: org.apache.spark.sql.DataFrame = [bookID: int, title: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val bookSchema = new StructType(Array(\n",
    "   new StructField(\"bookID\", IntegerType, false),\n",
    "   new StructField(\"title\", StringType, false),\n",
    "   new StructField(\"authors\", StringType, false),\n",
    "   new StructField(\"average_rating\", FloatType, false),\n",
    "   new StructField(\"isbn\", StringType, false),\n",
    "   new StructField(\"isbn13\", StringType, false),\n",
    "   new StructField(\"language_code\", StringType, false),\n",
    "   new StructField(\"num_pages\", IntegerType, false),\n",
    "   new StructField(\"ratings_count\", IntegerType, false),\n",
    "   new StructField(\"text_reviews_count\", IntegerType, false)))\n",
    "\n",
    "val booksDF = spark.read.format(\"csv\")\n",
    "                         .schema(bookSchema)\n",
    "                         .option(\"header\", \"true\")\n",
    "                         .load(\"../data/books.csv\")\n",
    "booksDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Determining the `null` elements\n",
    "\n",
    "* Here we can use the combination of `filter` and `row` to determine if anything is null\n",
    "* We see through this exploratory exercise that five rows are indeed `null`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+--------------+----+------+-------------+---------+-------------+------------------+\n",
      "|bookID|title|authors|average_rating|isbn|isbn13|language_code|num_pages|ratings_count|text_reviews_count|\n",
      "+------+-----+-------+--------------+----+------+-------------+---------+-------------+------------------+\n",
      "|  null| null|   null|          null|null|  null|         null|     null|         null|              null|\n",
      "|  null| null|   null|          null|null|  null|         null|     null|         null|              null|\n",
      "|  null| null|   null|          null|null|  null|         null|     null|         null|              null|\n",
      "|  null| null|   null|          null|null|  null|         null|     null|         null|              null|\n",
      "|  null| null|   null|          null|null|  null|         null|     null|         null|              null|\n",
      "+------+-----+-------+--------------+----+------+-------------+---------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "booksDF.filter(row => row.anyNull).show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `na`\n",
    "* Every `DataFrame` and `DataSet` has a method called `na`\n",
    "* Once referenced, `na` has methods that aid in cleaning our data\n",
    "* Here we can use `drop` (which has various signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+--------------+----+------+-------------+---------+-------------+------------------+\n",
      "|bookID|title|authors|average_rating|isbn|isbn13|language_code|num_pages|ratings_count|text_reviews_count|\n",
      "+------+-----+-------+--------------+----+------+-------------+---------+-------------+------------------+\n",
      "+------+-----+-------+--------------+----+------+-------------+---------+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cleanBooksDF: org.apache.spark.sql.DataFrame = [bookID: int, title: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleanBooksDF = booksDF.na.drop(how=\"any\")\n",
    "cleanBooksDF.filter(row => row.anyNull).show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Counting\n",
    "\n",
    "* Through the act of grouping data and counting that data we \n",
    "  can gain some other insight as to what needs to be cleaned\n",
    "* We find that the `language_code` is inconsistent, and we should replace various `en-*` to just `eng` to match the rest\n",
    "* We can use `na.replace` to replace the the values in a `Map[String, String]` where the key will be replaced with the value\n",
    "* This is how you would perform, `value_counts` from Pandas in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|language_code|count|\n",
      "+-------------+-----+\n",
      "|          fre|  209|\n",
      "|          zho|   16|\n",
      "|          glg|    4|\n",
      "|        en-CA|    9|\n",
      "|          rus|    7|\n",
      "|          nor|    1|\n",
      "|          ale|    1|\n",
      "|          cat|    3|\n",
      "|          ara|    2|\n",
      "|          por|   27|\n",
      "|          lat|    3|\n",
      "|          swe|    6|\n",
      "|          gla|    1|\n",
      "|          mul|   21|\n",
      "|          eng|10594|\n",
      "|          jpn|   64|\n",
      "|           nl|    7|\n",
      "|          grc|   12|\n",
      "|          dan|    1|\n",
      "|          srp|    1|\n",
      "|        en-GB|  341|\n",
      "|          heb|    1|\n",
      "|          tur|    3|\n",
      "|          enm|    3|\n",
      "|          msa|    1|\n",
      "|          wel|    1|\n",
      "|          ita|   19|\n",
      "|        en-US| 1699|\n",
      "|          spa|  419|\n",
      "|          ger|  238|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanBooksDF.groupBy(\"language_code\").count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|language_code|count|\n",
      "+-------------+-----+\n",
      "|          fre|  209|\n",
      "|          zho|   16|\n",
      "|          glg|    4|\n",
      "|          rus|    7|\n",
      "|          nor|    1|\n",
      "|          ale|    1|\n",
      "|          cat|    3|\n",
      "|          ara|    2|\n",
      "|          por|   27|\n",
      "|          lat|    3|\n",
      "|          swe|    6|\n",
      "|          gla|    1|\n",
      "|          mul|   21|\n",
      "|          eng|12643|\n",
      "|          jpn|   64|\n",
      "|           nl|    7|\n",
      "|          grc|   12|\n",
      "|          dan|    1|\n",
      "|          srp|    1|\n",
      "|          heb|    1|\n",
      "|          tur|    3|\n",
      "|          enm|    3|\n",
      "|          msa|    1|\n",
      "|          wel|    1|\n",
      "|          ita|   19|\n",
      "|          spa|  419|\n",
      "|          ger|  238|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "engCleanDF: org.apache.spark.sql.DataFrame = [bookID: int, title: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val engCleanDF = cleanBooksDF.na.replace(cols = Seq(\"language_code\"), \n",
    "                                         replacement = Map(\"en-US\" -> \"eng\", \n",
    "                                                           \"en-CA\" -> \"eng\", \n",
    "                                                           \"en-GB\" -> \"eng\"))\n",
    "engCleanDF.groupBy(\"language_code\").count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholds of `null`\n",
    "\n",
    "* You can specify in `drop` a threshold of a number of `null` in order to delete\n",
    "* Here we will create our own dataset to demonstrate. Rows vary, some will contain:\n",
    "  * All `null` or `NaN` values\n",
    "  * Some of `null` or `NaN` of various sizes\n",
    "  * Some with only one `null` or `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|                null|        null|       NaN|        NaN|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "|          Kazakhstan|         KZT|       NaN|        NaN|\n",
      "|            Tanzania|         TZA|       NaN|        NaN|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "|               China|        null|       NaN|        NaN|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n",
       "import spark.implicits._\n",
       "countrySeq: Seq[(String, String, Float, Float)] = List((Afghanistan,AFG,251830.0,3.4656032E7), (Algeria,DZA,919600.0,4.0606052E7), (Australia,AUS,2970000.0,2.412716E7), (Montenegro,MNE,5333.0,622781.0), (null,null,NaN,NaN), (Morocco,MAR,172414.0,3.3824768E7), (Syrian Arab Republic,SYR,71498.0,1.8430452E7), (Kazakhstan,KZT,NaN,NaN), (Tanzania,TZA,NaN,NaN), (Trinidad and Tobago,TTO,1981.0,1364962.0), (Ukraine,UKR,233062.0,4.5004644E7), (China,null,NaN,NaN))\n",
       "countryDF: org.apache.spark.sql.DataFrame = [country: string, abbreviation: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "import spark.implicits._\n",
    "val countrySeq:Seq[(String, String, Float, Float)] = Seq(\n",
    "   (\"Afghanistan\", \"AFG\", 251830, 34656032),\n",
    "   (\"Algeria\", \"DZA\", 919600, 40606052),\n",
    "   (\"Australia\", \"AUS\", 2970000, 24127159),\n",
    "   (\"Montenegro\", \"MNE\", 5333, 622781),\n",
    "   (null, null, Float.NaN, Float.NaN),\n",
    "   (\"Morocco\", \"MAR\", 172414, 33824769),\n",
    "   (\"Syrian Arab Republic\", \"SYR\", 71498, 18430453),\n",
    "   (\"Kazakhstan\", \"KZT\", Float.NaN, Float.NaN), \n",
    "   (\"Tanzania\", \"TZA\",Float.NaN, Float.NaN),\n",
    "   (\"Trinidad and Tobago\", \"TTO\", 1981, 1364962),\n",
    "   (\"Ukraine\", \"UKR\", 233062, 45004645),\n",
    "   (\"China\", null, Float.NaN, Float.NaN) \n",
    ")\n",
    "val countryDF = countrySeq.toDF(\"country\", \"abbreviation\", \"area_sq_mi\", \"population\")\n",
    "countryDF.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit to 2 `null` or `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "|          Kazakhstan|         KZT|       NaN|        NaN|\n",
      "|            Tanzania|         TZA|       NaN|        NaN|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryDF.na.drop(minNonNulls = 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit to 1 `null` or `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "|          Kazakhstan|         KZT|       NaN|        NaN|\n",
      "|            Tanzania|         TZA|       NaN|        NaN|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "|               China|        null|       NaN|        NaN|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryDF.na.drop(minNonNulls = 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing with `fill`\n",
    "\n",
    "* Imputing is defined as \"(of a value) assigned to something by inference from the value of the products or processes to which it contributes; estimated.\"\n",
    "* We can replace either `null` or `NaN` values with `0`, the `mean` of that column, or some other clever calculation, like imputing in a value based on another column like class, income, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace any values wholesale by provided a `Map[String,Any]` where the key is column name and the value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|             default|          --|      -1.0|       -1.0|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "|          Kazakhstan|         KZT|      -1.0|       -1.0|\n",
      "|            Tanzania|         TZA|      -1.0|       -1.0|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "|               China|          --|      -1.0|       -1.0|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "imputedDF: org.apache.spark.sql.DataFrame = [country: string, abbreviation: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val imputedDF = countryDF.na.fill(valueMap = Map( \"country\" -> \"default\", \n",
    "                                  \"abbreviation\" -> \"--\",\n",
    "                                  \"area_sq_mi\" -> -1,\n",
    "                                  \"population\" -> -1))\n",
    "imputedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up duplicates\n",
    "* We can clean up duplicates using `dropDuplicates` on `DataSet` or `DataFrame`\n",
    "* Let's create another country `DataFrame` with duplicates, then use `dropDuplicates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countrySeq: Seq[(String, String, Float, Float)] = List((Afghanistan,AFG,251830.0,3.4656032E7), (Algeria,DZA,919600.0,4.0606052E7), (Australia,AUS,2970000.0,2.412716E7), (Montenegro,MNE,5333.0,622781.0), (Montenegro,MNE,5333.0,622781.0), (Morocco,MAR,172414.0,3.3824768E7), (Syrian Arab Republic,SYR,71498.0,1.8430452E7), (Trinidad and Tobago,TTO,1981.0,1364962.0), (Ukraine,UKR,233062.0,4.5004644E7), (Algeria,DZA,919600.0,4.0606052E7))\n",
       "countryDF: org.apache.spark.sql.DataFrame = [country: string, abbreviation: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val countrySeq:Seq[(String, String, Float, Float)] = Seq(\n",
    "   (\"Afghanistan\", \"AFG\", 251830, 34656032),\n",
    "   (\"Algeria\", \"DZA\", 919600, 40606052),\n",
    "   (\"Australia\", \"AUS\", 2970000, 24127159),\n",
    "   (\"Montenegro\", \"MNE\", 5333, 622781),\n",
    "   (\"Montenegro\", \"MNE\", 5333, 622781),\n",
    "   (\"Morocco\", \"MAR\", 172414, 33824769),\n",
    "   (\"Syrian Arab Republic\", \"SYR\", 71498, 18430453),\n",
    "   (\"Trinidad and Tobago\", \"TTO\", 1981, 1364962),\n",
    "   (\"Ukraine\", \"UKR\", 233062, 45004645),\n",
    "   (\"Algeria\", \"DZA\", 919600, 40606052)\n",
    ")\n",
    "val countryDF = countrySeq.toDF(\"country\", \"abbreviation\", \"area_sq_mi\", \"population\")\n",
    "countryDF.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryDF.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Duplicates where only one or more columns should be considered\n",
    "\n",
    "* You can also select which columns should be taken into account\n",
    "* This will prove worth is there is different data for rows, but for the same entity\n",
    "* Here will create another `DataFrame` but different country names, just different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|          Montenegro|         MNE|    5133.0|   622000.0|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "|             Algeria|         DZA|  919800.0|4.0049224E7|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countrySeq: Seq[(String, String, Float, Float)] = List((Afghanistan,AFG,251830.0,3.4656032E7), (Algeria,DZA,919600.0,4.0606052E7), (Australia,AUS,2970000.0,2.412716E7), (Montenegro,MNE,5333.0,622781.0), (Montenegro,MNE,5133.0,622000.0), (Morocco,MAR,172414.0,3.3824768E7), (Syrian Arab Republic,SYR,71498.0,1.8430452E7), (Trinidad and Tobago,TTO,1981.0,1364962.0), (Ukraine,UKR,233062.0,4.5004644E7), (Algeria,DZA,919800.0,4.0049224E7))\n",
       "countryDF: org.apache.spark.sql.DataFrame = [country: string, abbreviation: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val countrySeq:Seq[(String, String, Float, Float)] = Seq(\n",
    "   (\"Afghanistan\", \"AFG\", 251830, 34656032),\n",
    "   (\"Algeria\", \"DZA\", 919600, 40606052),\n",
    "   (\"Australia\", \"AUS\", 2970000, 24127159),\n",
    "   (\"Montenegro\", \"MNE\", 5333, 622781),\n",
    "   (\"Montenegro\", \"MNE\", 5133, 622000),\n",
    "   (\"Morocco\", \"MAR\", 172414, 33824769),\n",
    "   (\"Syrian Arab Republic\", \"SYR\", 71498, 18430453),\n",
    "   (\"Trinidad and Tobago\", \"TTO\", 1981, 1364962),\n",
    "   (\"Ukraine\", \"UKR\", 233062, 45004645),\n",
    "   (\"Algeria\", \"DZA\", 919800, 40049222)\n",
    ")\n",
    "val countryDF = countrySeq.toDF(\"country\", \"abbreviation\", \"area_sq_mi\", \"population\")\n",
    "countryDF.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing a drop duplicates when data is different\n",
    "* In the above, although there are two `Montenegro` and two `Algeria`, they have different values\n",
    "* So let's experiment with `dropDuplicates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "|          Montenegro|         MNE|    5133.0|   622000.0|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "|             Algeria|         DZA|  919800.0|4.0049224E7|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryDF.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a `dropDuplicates` threshold\n",
    "\n",
    "* We can stipulate the columns to be accounted for when dropping columns \n",
    "* Check the [`dropDuplicates` API](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@dropDuplicates(col1:String,cols:String*):org.apache.spark.sql.Dataset[T]) in the `DataSet` class for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-----------+\n",
      "|             country|abbreviation|area_sq_mi| population|\n",
      "+--------------------+------------+----------+-----------+\n",
      "|           Australia|         AUS| 2970000.0| 2.412716E7|\n",
      "|             Algeria|         DZA|  919600.0|4.0606052E7|\n",
      "|             Ukraine|         UKR|  233062.0|4.5004644E7|\n",
      "|Syrian Arab Republic|         SYR|   71498.0|1.8430452E7|\n",
      "|             Morocco|         MAR|  172414.0|3.3824768E7|\n",
      "|         Afghanistan|         AFG|  251830.0|3.4656032E7|\n",
      "|          Montenegro|         MNE|    5333.0|   622781.0|\n",
      "| Trinidad and Tobago|         TTO|    1981.0|  1364962.0|\n",
      "+--------------------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryDF.dropDuplicates(\"country\", \"abbreviation\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
