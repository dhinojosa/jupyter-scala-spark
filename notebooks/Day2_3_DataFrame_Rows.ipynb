{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rows\n",
    "* Rows represent data that fits into the columns and types of the `DataFrame`\n",
    "* Rows themselves do not have schemas\n",
    "* If you create a Row manually, you must specify the values in the same order as the schema of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------------------+\n",
      "| id|firstName|  lastName|          department|\n",
      "+---+---------+----------+--------------------+\n",
      "|  1|    James|   Gosling|Coffee and Access...|\n",
      "|  2|    Guido|Van Rossum|Snakes, Lizards, ...|\n",
      "|  3|   Bjarne|Stroustrup|           Optometry|\n",
      "|  4|     John|  McCarthy|      Speech Therapy|\n",
      "+---+---------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.Row\n",
       "structType: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(firstName,StringType,false), StructField(lastName,StringType,false), StructField(department,StringType,false))\n",
       "data: Seq[org.apache.spark.sql.Row] = List([1,James,Gosling,Coffee and Accessories], [2,Guido,Van Rossum,Snakes, Lizards, Spiders], [3,Bjarne,Stroustrup,Optometry], [4,John,McCarthy,Speech Therapy])\n",
       "dataFrame: org.apache.spark.sql.DataFrame = [id: int, firstName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val structType = new StructType(Array(\n",
    "    StructField(\"id\", IntegerType, false),\n",
    "    StructField(\"firstName\", StringType, false),\n",
    "    StructField(\"lastName\", StringType, false),\n",
    "    StructField(\"department\", StringType, false)))\n",
    "\n",
    "val data = Seq(\n",
    "     Row(1, \"James\", \"Gosling\", \"Coffee and Accessories\"),\n",
    "     Row(2, \"Guido\", \"Van Rossum\", \"Snakes, Lizards, Spiders\"),\n",
    "     Row(3, \"Bjarne\", \"Stroustrup\", \"Optometry\"),\n",
    "     Row(4, \"John\", \"McCarthy\", \"Speech Therapy\")\n",
    "   )\n",
    "\n",
    "val dataFrame = spark.createDataFrame(spark.sparkContext.parallelize(data), structType)\n",
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a row or rows\n",
    "\n",
    "* `union` can add one or more rows together\n",
    "* `union` requires a `DataSet` of `Row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matz: (Int, String, String, String) = (5,Yukihiro,Matsumoto,Jewelry)\n",
       "wall: (Int, String, String, String) = (6,Larry,Wall,Jewelry)\n",
       "jewelers: org.apache.spark.sql.DataFrame = [id: int, firstName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matz = (5, \"Yukihiro\", \"Matsumoto\", \"Jewelry\")\n",
    "val wall = (6, \"Larry\", \"Wall\", \"Jewelry\")\n",
    "val jewelers = Seq(matz, wall).toDF(\"id\", \"firstName\", \"lastName\", \"department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Rows using `union` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------------------+\n",
      "| id|firstName|  lastName|          department|\n",
      "+---+---------+----------+--------------------+\n",
      "|  1|    James|   Gosling|Coffee and Access...|\n",
      "|  2|    Guido|Van Rossum|Snakes, Lizards, ...|\n",
      "|  3|   Bjarne|Stroustrup|           Optometry|\n",
      "|  4|     John|  McCarthy|      Speech Therapy|\n",
      "|  5| Yukihiro| Matsumoto|             Jewelry|\n",
      "|  6|    Larry|      Wall|             Jewelry|\n",
      "+---+---------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "combinedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, firstName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val combinedDF = dataFrame.union(jewelers)\n",
    "combinedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row: org.apache.spark.sql.Row = [1,James,Gosling,Coffee and Accessories]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val row = combinedDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array: Array[org.apache.spark.sql.Row] = Array([1,James,Gosling,Coffee and Accessories], [2,Guido,Van Rossum,Snakes, Lizards, Spiders], [3,Bjarne,Stroustrup,Optometry])\n",
       "res8: String = [1,James,Gosling,Coffee and Accessories][2,Guido,Van Rossum,Snakes, Lizards, Spiders][3,Bjarne,Stroustrup,Optometry]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val array = combinedDF.take(3)\n",
    "array.mkString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Elements of a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stair Repair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odersky: org.apache.spark.sql.Row = [7,Martin,Odersky,Stair Repair]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val odersky = Row(7, \"Martin\", \"Odersky\", \"Stair Repair\")\n",
    "println(odersky.getString(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kernighan: org.apache.spark.sql.Row = [7,Brian,Kernighan,Oceanic Travel]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kernighan = Row(7, \"Brian\", \"Kernighan\", \"Oceanic Travel\")\n",
    "println(kernighan.getAs[Int](0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spiders\n",
      "Spiders\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "timBernersLee: org.apache.spark.sql.Row = [8,Tim,Berners Lee,Spiders]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val timBernersLee = Row(8, \"Tim\", \"Berners Lee\", \"Spiders\")\n",
    "println(timBernersLee.apply(3))\n",
    "println(timBernersLee(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is no direct access to a row using an index\n",
    "\n",
    "* Spark is scaled across multiple nodes\n",
    "* Gaining a direct access to a row would be ineffective\n",
    "* Using generalized manipulation via `DataFrame` and functional programming would be how one would evoke change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Add Rows \n",
    "\n",
    "**Step 1:** Read from the `../data/students.csv` file\n",
    "\n",
    "**Step 2:** Determine the count of data using `count()` on the `DataFrame`\n",
    "\n",
    "**Step 3:** Take the following students, create a `DataFrame` from it, and add it to the `DataFrame` you just loaded\n",
    "\n",
    "```\n",
    "265-0983,95,93,96,86\n",
    "613-3461,80,81,82,87\n",
    "173-6835,81,91,85,72\n",
    "146-4885,81,88,99,94\n",
    "202-5972,86,97,93,80\n",
    "619-2192,88,93,89,76\n",
    "064-0847,91,82,81,74\n",
    "684-5156,94,97,91,83\n",
    "```\n",
    "\n",
    "**Step 4:** Determine the count of the additional data\n",
    "\n",
    "**Step 5:** Write the data to a file called `../data/mergedstudents.csv`\n",
    "\n",
    "**Step 6:** Reread the file back in and determine if the `count` is still the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
