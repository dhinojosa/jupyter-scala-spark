{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rows\n",
    "* Rows represent data that fits into the columns and types of the `DataFrame`\n",
    "* Rows themselves do not have schemas\n",
    "* If you create a Row manually, you must specify the values in the same order as the schema of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------------------+\n",
      "| id|firstName|  lastName|          department|\n",
      "+---+---------+----------+--------------------+\n",
      "|  1|    James|   Gosling|Coffee and Access...|\n",
      "|  2|    Guido|Van Rossum|Snakes, Lizards, ...|\n",
      "|  3|   Bjarne|Stroustrup|           Optometry|\n",
      "|  4|     John|  McCarthy|      Speech Therapy|\n",
      "+---+---------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.Row\n",
       "structType: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(firstName,StringType,false), StructField(lastName,StringType,false), StructField(department,StringType,false))\n",
       "data: Seq[org.apache.spark.sql.Row] = List([1,James,Gosling,Coffee and Accessories], [2,Guido,Van Rossum,Snakes, Lizards, Spiders], [3,Bjarne,Stroustrup,Optometry], [4,John,McCarthy,Speech Therapy])\n",
       "dataFrame: org.apache.spark.sql.DataFrame = [id: int, firstName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val structType = new StructType(Array(\n",
    "    StructField(\"id\", IntegerType, false),\n",
    "    StructField(\"firstName\", StringType, false),\n",
    "    StructField(\"lastName\", StringType, false),\n",
    "    StructField(\"department\", StringType, false)))\n",
    "\n",
    "val data = Seq(\n",
    "     Row(1, \"James\", \"Gosling\", \"Coffee and Accessories\"),\n",
    "     Row(2, \"Guido\", \"Van Rossum\", \"Snakes, Lizards, Spiders\"),\n",
    "     Row(3, \"Bjarne\", \"Stroustrup\", \"Optometry\"),\n",
    "     Row(4, \"John\", \"McCarthy\", \"Speech Therapy\")\n",
    "   )\n",
    "\n",
    "val dataFrame = spark.createDataFrame(spark.sparkContext.parallelize(data), structType)\n",
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a row or rows\n",
    "\n",
    "* `union` can add one or more rows together\n",
    "* `union` requires a `DataSet` of `Row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "matz: (Int, String, String, String) = (5,Yukihiro,Matsumoto,Jewelry)\n",
       "wall: (Int, String, String, String) = (6,Larry,Wall,Jewelry)\n",
       "jewelers: org.apache.spark.sql.DataFrame = [id: int, firstName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val matz = (5, \"Yukihiro\", \"Matsumoto\", \"Jewelry\")\n",
    "val wall = (6, \"Larry\", \"Wall\", \"Jewelry\")\n",
    "val jewelers = Seq(matz, wall).toDF(\"id\", \"firstName\", \"lastName\", \"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------------------+\n",
      "| id|firstName|  lastName|          department|\n",
      "+---+---------+----------+--------------------+\n",
      "|  1|    James|   Gosling|Coffee and Access...|\n",
      "|  2|    Guido|Van Rossum|Snakes, Lizards, ...|\n",
      "|  3|   Bjarne|Stroustrup|           Optometry|\n",
      "|  4|     John|  McCarthy|      Speech Therapy|\n",
      "|  5| Yukihiro| Matsumoto|             Jewelry|\n",
      "|  6|    Larry|      Wall|             Jewelry|\n",
      "+---+---------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "combinedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, firstName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val combinedDF = dataFrame.union(jewelers)\n",
    "combinedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row: org.apache.spark.sql.Row = [1,James,Gosling,Coffee and Accessories]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val row = combinedDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[org.apache.spark.sql.Row] = Array([1,James,Gosling,Coffee and Accessories], [2,Guido,Van Rossum,Snakes, Lizards, Spiders], [3,Bjarne,Stroustrup,Optometry])\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedDF.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Elements of a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odersky: org.apache.spark.sql.Row = [7,Martin,Odersky,Stair Repair]\n",
       "res10: String = Stair Repair\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val odersky = Row(7, \"Martin\", \"Odersky\", \"Stair Repair\")\n",
    "odersky.getString(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kernighan: org.apache.spark.sql.Row = [7,Brian,Kernighan,Oceanic Travel]\n",
       "res11: Int = 7\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kernighan = Row(7, \"Brian\", \"Kernighan\", \"Oceanic Travel\")\n",
    "kernighan.getAs[Int](0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the last two for a larger `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Employee(id:Int, )\n",
    "val jewelers = Seq().toDF(\"id\", \"firstName\", \"lastName\", \"department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all elements in an `Array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all the departments using functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "48: error: value toList is not a member of org.apache.spark.sql.Dataset[String]",
     "output_type": "error",
     "traceback": [
      "<console>:48: error: value toList is not a member of org.apache.spark.sql.Dataset[String]",
      "       combinedDF.map(row => row.getAs[String](3)).toList",
      "                                                   ^",
      ""
     ]
    }
   ],
   "source": [
    "combinedDF.map(row => row.getAs[String](3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
